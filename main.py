from fastapi import FastAPI, HTTPException, BackgroundTasks, Request, Response, status
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
from pathlib import Path
import asyncio
import shutil
import gzip
import os
import psycopg2
import psycopg2.extras
import base64
import bcrypt
import jwt
from fastapi.middleware.cors import CORSMiddleware
from dotenv import load_dotenv
import logging
import io

logger = logging.getLogger("reports")
load_dotenv()

# =========================================================
# CONFIGURATION & METADATA
# =========================================================

# FILE METADATA - Maps display_name to DB configuration
FILE_METADATA = {
    "CCOD": {
        "tableName": "ccod_bal_daily",
        "procedureName": "ccod_bal_upd",
        "useFilePath": False,
        "passFilePathToProcedure": True,
        "scriptType": "base"
    },

    "TDS REPORT": {   # ‚ö† match frontend exactly
        "tableName": "tds_daily",
        "sequenceName": "tds_daily_rno_seq",
        "procedureName": "tds_upd",
        "useFilePath": False,
        "scriptType": "base"
    },

    "Montrial": {
        "tableName": "mon_trial_daily",
        "sequenceName": "mon_trial_daily_rno_seq",
        "procedureName": "mon_trial_load",
        "useFilePath": False,
        "scriptType": "custom"
    },
    "SHADOW_DEP": {
    "tableName": "shadow_dep",
    "sequenceName": "shadow_dep_rno_seq",
    "procedureName": "shadow_dep_upd_2",
    "useFilePath": False,
    "scriptType": "custom"
    },

    "SHADOW_LOAN": {
        "tableName": "shadow_loan",
        "sequenceName": "shadow_loan_rno_seq",
        "procedureName": "shadow_loan_upd_2",
        "useFilePath": False,
        "scriptType": "custom"
    },
}


# BASE SCRIPT - Used for simple files (CCOD, TDS_Report, etc.)
# BASE_SCRIPT = [
#     "TRUNCATE TABLE {{TABLE_NAME}};",
#     "ALTER SEQUENCE {{SEQUENCE_NAME}} RESTART WITH 1;",
#     "COPY {{TABLE_NAME}}(fulltext) FROM STDIN WITH (FORMAT text, ENCODING 'UTF8');",
#     "CALL {{CALL_STATEMENT}};"
# ]

BASE_SCRIPT = [
    "TRUNCATE TABLE {{TABLE_NAME}};",
    "COPY {{TABLE_NAME}}(fulltext) FROM STDIN WITH (FORMAT text, ENCODING 'UTF8');",
    "{{CALL_PROCEDURE}}"
]


CUSTOM_SCRIPTS = {
    "MONTRIAL": [

              # 1Ô∏è‚É£ Truncate
             "TRUNCATE TABLE mon_trial_daily;",
             # 2Ô∏è‚É£ Reset sequence
             "ALTER SEQUENCE mon_trial_daily_rno_seq RESTART WITH 1;",
#             # 3Ô∏è‚É£ COPY (file_path used here clearly)
             f"""
             COPY mon_trial_daily(fulltext)
             FROM STDIN
             WITH (
                 FORMAT text,
                 ENCODING 'UTF8'
             );
             """,
             # 4Ô∏è‚É£ Cleanup deletes
             "delete from mon_trial_daily where substr(fulltext,3,10) ='B: GLAJ02 ';",
             "delete from mon_trial_daily where substr(fulltext,3,10) ='EQUEST PAR';",
             "delete from mon_trial_daily where substr(fulltext,3,10) ='PORT      ';",             "delete from mon_trial_daily where substr(fulltext,3,10) ='TE CODE   ';",
             "delete from mon_trial_daily where substr(fulltext,3,10) ='QUESTOR   ';",
             "delete from mon_trial_daily where substr(fulltext,3,10) ='TE        ';",
             "delete from mon_trial_daily where substr(fulltext,3,10) ='AR        ';",
             "delete from mon_trial_daily where trim(fulltext) ='01';",
             "delete from mon_trial_daily where substr(trim(fulltext),1,6) ='CURREN';",
             "delete from mon_trial_daily where SUBSTR(fulltext,51,8) ='FOR YEAR';",
             "delete from mon_trial_daily where SUBSTR(fulltext,13,12) ='1          N';",
             "delete from mon_trial_daily where SUBSTR(fulltext,13,10) ='UNIT TOTAL';",
             "delete from mon_trial_daily where SUBSTR(fulltext,13,14) ='CURRENCY TOTAL';",
             "delete from mon_trial_daily where SUBSTR(fulltext,13,12) ='ENTITY TOTAL';",
             "delete from mon_trial_daily where fulltext IS NULL;",
             "delete from mon_trial_daily where trim(fulltext)='.00';",
             "delete from mon_trial_daily where trim(fulltext)='UNIT TOTAL';",
             # 5Ô∏è‚É£ Procedure
             "CALL mon_trial_load();",
             # 6Ô∏è‚É£ Final insert
             """
             INSERT INTO MONTRIAL
             (BRCODE, CGL, CGLDESC, OPEN_BAL, DEBIT_BAL, CREDIT_BAL, NET_CHANGE, END_BAL, DATE_OF_REPORT)             SELECT
                 BRCD::NUMERIC,
                 CGL::NUMERIC,
                 CGLDESC,
                 OPEN_BAL,
                 DEBIT_BAL,
                 CREDIT_BAL,
                 NET_CHANGE,
                 END_BAL,
                 REPORT_DT
             FROM MON_TRIAL_DAILY
             WHERE END_BAL IS NOT NULL;
             """,
             # 7Ô∏è‚É£ Final procedure
             "CALL mon_trial_drcr();"
         ],
         # ==========================================
    # SHADOW_DEP
    # ==========================================
    "SHADOW_DEP": [

        "TRUNCATE TABLE {{TABLE_NAME}};",
        "ALTER SEQUENCE {{SEQUENCE_NAME}} RESTART WITH 1;",

        """
        COPY {{TABLE_NAME}}(fulltext)
        FROM STDIN
        WITH (
            FORMAT text,
            ENCODING 'UTF8'
        );
        """,

        "{{CALL_PROCEDURE}}"
    ],


    # ==========================================
    # SHADOW_LOAN
    # ==========================================
    "SHADOW_LOAN": [

        "TRUNCATE TABLE {{TABLE_NAME}};",
        "ALTER SEQUENCE {{SEQUENCE_NAME}} RESTART WITH 1;",

        """
        COPY {{TABLE_NAME}}(fulltext)
        FROM STDIN
        WITH (
            FORMAT text,
            ENCODING 'UTF8'
        );
        """,

        "{{CALL_PROCEDURE}}"
    ]
}

# DB Config
DB_HOST = os.getenv("DB_HOST")
DB_PORT = os.getenv("DB_PORT")
DB_NAME = os.getenv("DB_NAME")
DB_USER = os.getenv("DB_USER")
DB_PASSWORD = os.getenv("DB_PASSWORD")
JWT_SECRET = os.getenv("JWT_SECRET", "default-secret-key")
JWT_ALGORITHM = "HS256"
JWT_EXPIRE_MINUTES = 60

app = FastAPI()

CORS_ORIGINS = ["http://localhost:5000"]
app.add_middleware(
    CORSMiddleware,
    allow_origins=CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["Authorization"],
)


# =========================================================
# DYNAMIC SCRIPT GENERATION - CORE LOGIC
# =========================================================

class ScriptGenerator:
    """
    Generates SQL scripts dynamically based on file metadata.
    Supports both BASE_SCRIPT and CUSTOM_SCRIPTS templates.
    """

    @staticmethod
    def get_script(display_name: str, file_path: str) -> List[str]:
        """
        HERE: Main entry point for script generation
        
        Args:
            display_name: File display name (e.g., "Montrial", "CCOD")
            file_path: Full path to the file being processed
            
        Returns:
            List of SQL statements to execute
            
        Raises:
            ValueError: If display_name not found in FILE_METADATA
        """
        # Validate file exists in metadata
        if display_name not in FILE_METADATA:
            raise ValueError(
                f"‚ùå Unknown file type: {display_name}. "
                f"Available: {', '.join(FILE_METADATA.keys())}"
            )

        metadata = FILE_METADATA[display_name]
        script_type = metadata.get("scriptType", "base")

        # Route to appropriate template
        if script_type == "custom":
            return ScriptGenerator._get_custom_script(display_name, metadata, file_path)
        else:
            return ScriptGenerator._get_base_script(display_name, metadata, file_path)

    @staticmethod
    def _get_base_script(display_name: str, metadata: Dict, file_path: str) -> List[str]:
        """
        HERE: Base script generation - simple template-based approach
        Used for straightforward files: CCOD, TDS_Report, etc.
        """
        script = BASE_SCRIPT.copy()
        
        # Build FILE_PATH_OR_STDIN placeholder
        if metadata.get("useFilePath", False):
            call_stmt = f"CALL {metadata.get('procedureName')}('{file_path}');"
        else:
            call_stmt = f"CALL {metadata.get('procedureName')}();"


        # Replace all placeholders
        return ScriptGenerator._replace_placeholders(
            script,
            display_name,
            metadata,
            file_path
        )

    @staticmethod
    def _get_custom_script(display_name: str, metadata: Dict, file_path: str) -> List[str]:
        """
        HERE: Custom script generation - uses CUSTOM_SCRIPTS templates
        Used for complex files: MONTRIAL, SHADOW_DEP, etc.
        """
        # Map display_name to custom script key
        custom_key = display_name.upper()
        
        if custom_key not in CUSTOM_SCRIPTS:
            # Fallback to base script if custom not found
            logger.warning(
                f"‚ö†Ô∏è No custom script for {custom_key}, using BASE_SCRIPT"
            )
            return ScriptGenerator._get_base_script(display_name, metadata, file_path)

        script = CUSTOM_SCRIPTS[custom_key].copy()

        # Build FILE_PATH_OR_STDIN placeholder
        if metadata.get("useFilePath", False):
            file_path_placeholder = f"'{file_path}'"
        else:
            file_path_placeholder = "STDIN"

        # Replace all placeholders
        return ScriptGenerator._replace_placeholders(
            script,
            display_name,
            metadata,
            file_path_placeholder
        )

    @staticmethod
    def _replace_placeholders(base_script, display_name, metadata, file_path):
        statements = []

        procedure = metadata.get("procedureName")
        table_name = metadata.get("tableName")
        sequence_name = metadata.get("sequenceName")

        # Build CALL statement
        if procedure:
            if metadata.get("passFilePathToProcedure", False):
                call_stmt = f"CALL {procedure}('{file_path}');"
            else:
                call_stmt = f"CALL {procedure}();"
        else:
            call_stmt = ""

        for stmt in base_script:
            statement = stmt

            # Replace TABLE
            if table_name:
                statement = statement.replace(
                    "{{TABLE_NAME}}",
                    table_name
                )

            # Replace SEQUENCE (only if exists)
            if sequence_name:
                statement = statement.replace(
                    "{{SEQUENCE_NAME}}",
                    sequence_name
                )
            else:
                # If metadata doesn't have sequence,
                # remove the ALTER SEQUENCE line completely
                if "{{SEQUENCE_NAME}}" in statement:
                    continue  # skip this statement

            # Replace CALL
            if "{{CALL_PROCEDURE}}" in statement:
                statement = statement.replace(
                    "{{CALL_PROCEDURE}}",
                    call_stmt
                )

            statements.append(statement)

        return statements






# =========================================================
# REQUEST MODELS
# =========================================================

class FileItem(BaseModel):
    key: str
    name: str
    display_name: str  # THIS IS CRITICAL - matches FILE_METADATA keys
    files: int
    file_type: str
    db: str


class FileRequest(BaseModel):
    fromPath: str
    toPath: str
    startDate: str
    endDate: str
    files: List[FileItem]


# =========================================================
# UTILITIES
# =========================================================

def get_db_connection():
    return psycopg2.connect(
        host=DB_HOST,
        port=DB_PORT,
        database=DB_NAME,
        user=DB_USER,
        password=DB_PASSWORD
    )


def delete_all_files_in_directory(path: str):
    if not os.path.exists(path):
        return
    for f in Path(path).glob("*"):
        if f.is_file():
            f.unlink()
        elif f.is_dir():
            shutil.rmtree(f)


def unzip_file(source: Path, dest: Path) -> Path:
    """Unzip .gz recursively"""
    current = source
    dest.mkdir(parents=True, exist_ok=True)

    while current.suffix == ".gz":
        out_file = dest / current.stem
        if out_file.exists():
            return out_file
        with gzip.open(current, "rb") as f_in, open(out_file, "wb") as f_out:
            shutil.copyfileobj(f_in, f_out)
        current = out_file

    return current

# =========================================================
# FILE CLEANER (Same as Java logic)
# =========================================================

def copy_file_with_encoding(source_file: Path, destination_file: Path):
    """
    Clean file before DB COPY:
    - Remove null characters
    - Remove non-ASCII characters
    - Remove quote-like characters
    - Remove empty lines
    - Replace backslashes and tabs
    """

    try:
        with open(source_file, "r", encoding="ISO-8859-1", errors="ignore") as f:
            lines = f.readlines()

        cleaned_lines = []

        for line in lines:
            line = line.replace("\x00", " ")
            line = "".join(c if ord(c) < 128 else " " for c in line)
            line = line.replace('""', " ")

            for ch in ['"', "'", "‚Äò", "‚Äô", "‚Äö", "‚Äõ", "`", "¬¥"]:
                line = line.replace(ch, " ")

            line = line.replace("\\", " ").replace("\t", " ")

            if line.strip():
                cleaned_lines.append(line)

        content = "\n".join(cleaned_lines)

        with open(destination_file, "w", encoding="ISO-8859-1") as f:
            f.write(content)

    except Exception as e:
        raise RuntimeError(f"Error cleaning file: {str(e)}")


def send_status(msg_type: str, msg: str):
    """Simulate WebSocket message"""
    logging.info(f"[{msg_type}] {msg}")


# =========================================================
# DATABASE PROCESSING
# =========================================================

def run_db_script(file_path: str, display_name: str) -> str:
    """
    HERE: Execute dynamically generated SQL script
    
    Args:
        file_path: Full path to data file
        display_name: File display name (e.g., "Montrial")
        
    Returns:
        Success message or error detail
    """
    try:
        # HERE: Generate dynamic script
        sqls = ScriptGenerator.get_script(display_name, file_path)

        conn = get_db_connection()
        cur = conn.cursor()

        schema = os.getenv("BRANCH_SCHEMA", "public")
        cur.execute(f"SET search_path TO {schema}")

        for sql in sqls:
            sql_clean = sql.strip()
            # HERE: Handle COPY with STDIN specially
            if sql_clean.upper().startswith("COPY"):
                with open(file_path, 'rb') as f:
                    content = f.read()
                # Remove null bytes
                content = content.replace(b'\x00', b'')
                # Normalize line endings to LF
                content = content.replace(b'\r\n', b'\n').replace(b'\r', b'\n')
                buffer = io.BytesIO(content)
                cur.copy_expert(sql_clean, buffer)
            else:
                # Execute normal SQL
                cur.execute(sql_clean)

        conn.commit()
        cur.close()
        conn.close()

        msg = f"‚úÖ DB Processing completed for {display_name} ({schema})"
        send_status("success", msg)
        return msg

    except ValueError as e:
        # Script generation error
        error_msg = f"‚ùå Script generation error for {display_name}: {str(e)}"
        send_status("error", error_msg)
        raise HTTPException(status_code=400, detail=str(e))
        
    except Exception as e:
        # Database execution error
        error_msg = f"‚ùå DB error for {display_name}: {str(e)}"
        send_status("error", error_msg)
        logger.error(error_msg)
        raise HTTPException(status_code=500, detail=str(e))


async def process_file(file: FileItem, request: FileRequest):
    """
    HERE: Process single file from request
    Matches files by date range and file type, then executes DB script
    """
    matched_files = []
    start = datetime.strptime(request.startDate, "%Y-%m-%d").date()
    end = datetime.strptime(request.endDate, "%Y-%m-%d").date()
    src_dir = Path(request.fromPath)
    dest_dir = Path(request.toPath)
    dest_dir.mkdir(parents=True, exist_ok=True)

    if src_dir.resolve() == dest_dir.resolve():
        raise RuntimeError("fromPath and toPath must be different folders")

    # Iterate through date-based folders
    for folder in os.listdir(src_dir):
        folder_date = None
        try:
            folder_date = datetime.strptime(folder, "%Y%m%d").date()
        except:
            continue

        if not (start <= folder_date <= end):
            continue

        folder_path = src_dir / folder
        
        # Match files by prefix
        for f in folder_path.glob("*"):
            if file.file_type == "prefix" and f.name.lower().startswith(file.key.lower()):
                actual_file = f
                
                # Unzip if needed
                if f.suffix == ".gz":
                    actual_file = unzip_file(f, dest_dir)
                
                # Copy to destination
                dest_file = dest_dir / actual_file.name
                if actual_file.parent.resolve() != dest_dir.resolve():   # call cleaning function 
                    shutil.copy(actual_file, dest_file)
                    copy_file_with_encoding(actual_file, dest_file)

                # HERE: Execute dynamic DB script
                # IMPORTANT: Use display_name to lookup metadata
                result = run_db_script(str(dest_file), file.display_name)
                matched_files.append(result)

                if file.files == 1:
                    break

    if not matched_files:
        msg = f"‚ùå No matching file for {file.display_name}"
        send_status("error", msg)
        matched_files.append(msg)

    return matched_files


# =========================================================
# ENDPOINTS
# =========================================================

@app.get("/signin")
async def signin(request: Request, response: Response, code: str | None = None):
    """
    Existing signin endpoint - unchanged
    """
    try:
        auth_header = request.headers.get("Authorization")
        if not auth_header or not auth_header.startswith("Basic "):
            raise HTTPException(
                status_code=401,
                detail="Missing or invalid Authorization header"
            )
        encoded = auth_header.split(" ")[1]
        decoded = base64.b64decode(encoded).decode("utf-8")
        username, password = decoded.split(":", 1)

        conn = get_db_connection()
        cur = conn.cursor()
        cur.execute("""
            SELECT id, employeeid, username, password, brname, branchno, city, role, status, mis, slbc, reconciliation, expiry_date 
            FROM userloginrj WHERE employeeid=%s
        """, (username,))
        user_row = cur.fetchone()
        cur.close()
        conn.close()

        if not user_row:
            raise HTTPException(status_code=404, detail="User not found")

        columns = ["id", "employeeid", "username", "password", "brname", "branchno", "city", "role", "status", "mis", "slbc", "reconciliation", "expiry_date"]
        user = dict(zip(columns, user_row))

        hashed_password = user["password"]
        if isinstance(hashed_password, str):
            hashed_password = hashed_password.encode("utf-8")
        if not bcrypt.checkpw(password.encode("utf-8"), hashed_password):
            raise HTTPException(status_code=401, detail="Wrong password")

        now_ts = int(datetime.utcnow().timestamp())
        exp_ts = int((datetime.utcnow() + timedelta(hours=3)).timestamp())
        payload = {
            "iss": "misreports",
            "sub": "JWT Token",
            "username": user["username"],
            "role": user["role"],
            "schema": "public",
            "iat": now_ts,
            "exp": exp_ts
        }
        token = jwt.encode(payload, JWT_SECRET, algorithm="HS256")
        response.headers["Authorization"] = f"Bearer {token}"

        return {
            "error": False,
            "status_code": 202,
            "token": token,
            "employeeid": str(user["employeeid"]),
            "username": user["username"],
            "brname": user["brname"],
            "branchno": user["branchno"],
            "city": user["city"],
            "role": user["role"],
            "status": user["status"],
            "mis": user["mis"],
            "slbc": user["slbc"],
            "reconciliation": user["reconciliation"],
            "expiry_date": str(user["expiry_date"]),
            "schema": "public",
            "code": code
        }

    except HTTPException as e:
        raise e
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/paths")
def get_paths() -> dict:
    """Existing paths endpoint - unchanged"""
    from_paths_raw = os.environ.get("FROM_PATHS", "").strip()
    if not from_paths_raw:
        raise RuntimeError("Environment variable 'FROM_PATHS' is missing or empty.")

    from_paths = [p.strip() for p in from_paths_raw.split(",") if p.strip()]
    if not from_paths:
        raise RuntimeError("Environment variable 'FROM_PATHS' is empty after parsing.")

    from_path_str = ",".join(from_paths)

    to_path_raw = os.environ.get("TO_PATH", "").strip()
    if not to_path_raw:
        raise RuntimeError("Environment variable 'TO_PATH' is missing or empty.")

    return {
        "fromPath": from_path_str,
        "toPath": to_path_raw
    }


# =========================================================
# REPORTS ENDPOINT - QUERY MAPPING
# =========================================================

QUERY_MAP = {
    "GEN001": """
        SELECT
          ROW_NUMBER() OVER () AS serial_no,
          report_date,
          report_type,
          procedure_name,
          file_name,
          inserted_records,
          TO_CHAR(execution_time AT TIME ZONE 'Asia/Kolkata',
                  'DD Mon YYYY HH12:MI:SS AM') AS execution_time,
          TO_CHAR(end_time AT TIME ZONE 'Asia/Kolkata',
                  'DD Mon YYYY HH12:MI:SS AM') AS end_time,
          status,
          ROUND(EXTRACT(EPOCH FROM (end_time - execution_time))::NUMERIC, 2)
            AS time_in_seconds,
          error_count,
          COUNT(*) OVER() AS total_rows
        FROM procedure_log
        WHERE report_date BETWEEN %s AND %s
    """
}

# =========================================================
# REPORTS REQUEST MODELS
# =========================================================

class Filter(BaseModel):
    """Filter criteria for reports"""
    id: str
    value: str


class ReportRequest(BaseModel):
    """Request model for /reports endpoint"""
    reportType: str
    branch: str
    fromDate: str
    toDate: str
    page: int = 1
    size: int = 10
    sortBy: Optional[str] = None
    sortDirection: Optional[str] = "ASC"
    filters: Optional[List[Filter]] = []


# =========================================================
# REPORTS UTILITIES
# =========================================================

def format_date(date_str: str) -> str:
    """Format date from YYYY-MM-DD to DD-MM-YYYY"""
    try:
        return datetime.strptime(date_str, "%Y-%m-%d").strftime("%d-%m-%Y")
    except ValueError:
        return date_str


def apply_filters(query: str, filters: List[Filter]) -> str:
    """
    Apply filters to query
    
    HERE: Dynamic WHERE clause generation from filters
    """
    if not filters:
        return query

    clauses = []
    for f in filters:
        # HERE: Safe filtering with CAST and LIKE
        clauses.append(
            f"CAST({f.id} AS TEXT) LIKE '%{f.value}%'"
        )

    return f"SELECT * FROM ({query}) t WHERE " + " AND ".join(clauses)


def add_pagination_sorting(
    query: str,
    page: int,
    size: int,
    sort_by: Optional[str],
    sort_dir: Optional[str]
) -> str:
    """
    Add pagination and sorting to query
    
    HERE: size = -1 means no pagination (get all records)
    """
    if sort_by:
        query += f" ORDER BY {sort_by} {sort_dir}"

    # HERE: size = -1 means no pagination
    if size != -1:
        offset = max((page - 1) * size, 0)
        query += f" LIMIT {size} OFFSET {offset}"

    return query


# =========================================================
# REPORTS ENDPOINT
# =========================================================

@app.post("/reports")
def get_reports(request: ReportRequest) -> List[Dict[str, Any]]:
    """
    HERE: REPORTS ENDPOINT - Get procedure logs
    
    Supports:
    - Multiple report types (GEN001, etc.)
    - Date range filtering
    - Custom filters
    - Pagination & sorting
    - Schema/branch validation
    
    Flow:
    1. Validate branch
    2. Get base SQL query
    3. Apply filters
    4. Add pagination/sorting
    5. Execute & return results
    """
    try:
        # -------------------------------------------------
        # Branch validation
        # -------------------------------------------------
        if request.branch == "":
            raise ValueError("Branch is empty")

        # HERE: Branch parsed but NOT used in query (can be added if needed)
        branch = int(request.branch)

        # -------------------------------------------------
        # Get SQL query
        # -------------------------------------------------
        base_query = QUERY_MAP.get(request.reportType)
        if not base_query:
            raise HTTPException(
                status_code=400,
                detail=f"Invalid report type: {request.reportType}. Available: {', '.join(QUERY_MAP.keys())}"
            )

        # -------------------------------------------------
        # Date formatting
        # -------------------------------------------------
        from_date = format_date(request.fromDate) if request.fromDate else None
        to_date = format_date(request.toDate) if request.toDate else None

        # -------------------------------------------------
        # Apply filters
        # -------------------------------------------------
        final_query = apply_filters(base_query, request.filters)

        # -------------------------------------------------
        # Add pagination & sorting
        # -------------------------------------------------
        final_query = add_pagination_sorting(
            final_query,
            request.page,
            request.size,
            request.sortBy,
            request.sortDirection
        )

        # -------------------------------------------------
        # Execute query
        # -------------------------------------------------
        conn = psycopg2.connect(
            host=DB_HOST,
            port=DB_PORT,
            database=DB_NAME,
            user=DB_USER,
            password=DB_PASSWORD
        )

        try:
            with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
                # HERE: Pass dates as parameters for safety
                cur.execute(final_query, (from_date, to_date))
                result = cur.fetchall()
                return result
        finally:
            conn.close()

    except psycopg2.errors.QueryCanceled:
        logger.error(f"Query timeout for {request.reportType}")
        raise HTTPException(
            status_code=408,
            detail="Request timed out. Please try again with different filters."
        )

    except psycopg2.errors.SyntaxError as e:
        logger.error(f"SQL Syntax error: {str(e)}")
        raise HTTPException(status_code=404, detail=f"SQL Error: {str(e)}")

    except ValueError as e:
        logger.error(f"Validation error: {str(e)}")
        raise HTTPException(status_code=400, detail=f"Invalid Arguments: {str(e)}")

    except Exception as e:
        logger.error(f"Unexpected error in /reports: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Unexpected error: {str(e)}"
        )


# =========================================================
# COPY ENDPOINT (MAIN FILE PROCESSING)
# =========================================================

@app.post("/copy")
async def copy_endpoint(request: FileRequest, background_tasks: BackgroundTasks):
    """
    HERE: MAIN COPY ENDPOINT - Now fully dynamic
    
    Flow:
    1. Validate request
    2. Clear destination directory
    3. Process each file with dynamic script generation
    4. Return results
    """
    if not request.files:
        raise HTTPException(status_code=400, detail="At least one file must be selected")
    print(request)
    delete_all_files_in_directory(request.toPath)

    # Process all files concurrently
    results = await asyncio.gather(
        *(process_file(f, request) for f in request.files),
        return_exceptions=True  # Catch individual file errors
    )
    

    # Handle exceptions in results
    flat_results = []
    
    for result in results:
        if isinstance(result, Exception):
            flat_results.append(f"‚ùå Errorr: {str(result)}")
        elif isinstance(result, list):
            flat_results.extend(result)
        else:
            flat_results.append(str(result))

    # Dashboard refresh for special files
    if any(f.display_name.upper() == "MONTRIAL" for f in request.files):
        background_tasks.add_task(
            lambda: send_status("info", "Dashboard refreshed for MONTRIAL")
        )

    return {"status": "completed", "results": flat_results}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=5000, reload=True)  





    #===============
    #FOR ONLY MONTIAL
    #===============




    # from fastapi import FastAPI, HTTPException, BackgroundTasks, Request, Response
# from pydantic import BaseModel
# from typing import List, Dict, Any, Optional
# from datetime import datetime, timedelta
# from pathlib import Path
# import asyncio
# import shutil
# import gzip
# import os
# import psycopg2
# import psycopg2.extras
# import base64
# import bcrypt
# import jwt
# from fastapi.middleware.cors import CORSMiddleware
# from dotenv import load_dotenv
# import logging
# import io
# from pydantic import BaseModel, Field
# from typing import List, Dict, Any, Optional
# from datetime import datetime
# import asyncpg
# import logging

# logger = logging.getLogger("reports")
# load_dotenv()  # Load .env file

# # DB Config
# DB_HOST = os.getenv("DB_HOST")
# DB_PORT = os.getenv("DB_PORT")
# DB_NAME = os.getenv("DB_NAME")
# DB_USER = os.getenv("DB_USER")
# DB_PASSWORD = os.getenv("DB_PASSWORD")
# JWT_SECRET = os.getenv("JWT_SECRET", "default-secret-key")
# JWT_ALGORITHM = "HS256"
# JWT_EXPIRE_MINUTES = 60  # Token valid for 60 minutes

# app = FastAPI()


# CORS_ORIGINS = ["http://localhost:5000"]
# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=CORS_ORIGINS,   # must be list of exact origins
#     allow_credentials=True,       # only works with exact origins
#     allow_methods=["*"],
#     allow_headers=["*"],
#     expose_headers=["Authorization"],  
# )


# def get_db_connection():
#     return psycopg2.connect(
#         host=DB_HOST,
#         port=DB_PORT,
#         database=DB_NAME,
#         user=DB_USER,
#         password=DB_PASSWORD
#     )

# @app.get("/signin")
# async def signin(request: Request, response: Response, code: str | None = None):
#     try:
#         # 1Ô∏è‚É£ Decode Basic Auth
#         auth_header = request.headers.get("Authorization")
#         if not auth_header or not auth_header.startswith("Basic "):
#             raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Missing or invalid Authorization header")
#         encoded = auth_header.split(" ")[1]
#         decoded = base64.b64decode(encoded).decode("utf-8")
#         username, password = decoded.split(":", 1)

#         # 2Ô∏è‚É£ Fetch user from DB
#         conn = get_db_connection()
#         cur = conn.cursor()
#         cur.execute("""
#             SELECT id, employeeid, username, password, brname, branchno, city, role, status, mis, slbc, reconciliation, expiry_date 
#             FROM userloginrj WHERE employeeid=%s
#         """, (username,))
#         user_row = cur.fetchone()
#         cur.close()
#         conn.close()

#         if not user_row:
#             raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="User not found")
#         # print(user_row)

#         # 3Ô∏è‚É£ Map tuple to dict
#         columns = ["id", "employeeid", "username", "password", "brname", "branchno", "city", "role", "status", "mis", "slbc", "reconciliation", "expiry_date"]
#         user = dict(zip(columns, user_row))
#         # print(columns)

#         # 4Ô∏è‚É£ Verify password
#         hashed_password = user["password"]
#         if isinstance(hashed_password, str):
#             hashed_password = hashed_password.encode("utf-8")
#         if not bcrypt.checkpw(password.encode("utf-8"), hashed_password):
#             raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Wrong password")

#         # 5Ô∏è‚É£ Generate REAL JWT token
#         now_ts = int(datetime.utcnow().timestamp())
#         exp_ts = int((datetime.utcnow() + timedelta(hours=3)).timestamp())
#         expire = datetime.utcnow() + timedelta(hours=3)
#         payload = {
#             "iss": "misreports",
#             "sub": "JWT Token",
#             "username": user["username"],
#             "role": user["role"],
#             "schema": "public",
#             "iat": now_ts,
#             "exp": exp_ts
#         }
#         token = jwt.encode(payload, JWT_SECRET, algorithm="HS256")

#         response.headers["Authorization"] = f"Bearer {token}"

#         # 6Ô∏è‚É£ Return response
#         return {
#             "error": False,
#             "status_code": 202,
#             "token": token,
#             "employeeid": str(user["employeeid"]),
#             "username": user["username"],
#             "brname": user["brname"],
#             "branchno": user["branchno"],
#             "city": user["city"],
#             "role": user["role"],
#             "status": user["status"],
#             "mis": user["mis"],
#             "slbc": user["slbc"],
#             "reconciliation": user["reconciliation"],
#             "expiry_date": str(user["expiry_date"]),
#             "schema": "public",
#             "code": code
#         }

#     except HTTPException as e:
#         raise e
#     except Exception as e:
#         raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(e))

# @app.get("/paths")
# def get_paths() -> dict:
#     """Return loader path configuration from env (FROM_PATHS, TO_PATH)."""

#     # FROM_PATHS
#     from_paths_raw = os.environ.get("FROM_PATHS", "").strip()
#     if not from_paths_raw:
#         raise RuntimeError("Environment variable 'FROM_PATHS' is missing or empty.")

#     from_paths = [p.strip() for p in from_paths_raw.split(",") if p.strip()]
#     if not from_paths:
#         raise RuntimeError("Environment variable 'FROM_PATHS' is empty after parsing.")

#     from_path_str = ",".join(from_paths)  # no extra spaces

#     # TO_PATH
#     to_path_raw = os.environ.get("TO_PATH", "").strip()
#     if not to_path_raw:
#         raise RuntimeError("Environment variable 'TO_PATH' is missing or empty.")

#     return {
#         "fromPath": from_path_str,
#         "toPath": to_path_raw
#     }


# class FileItem(BaseModel):
#     key: str
#     name: str
#     display_name: str
#     files: int
#     file_type: str
#     db: str

# class FileRequest(BaseModel):
#     fromPath: str
#     toPath: str
#     startDate: str
#     endDate: str
#     files: List[FileItem]

# # --------------------
# # Utilities
# # --------------------
# def delete_all_files_in_directory(path: str):
#     if not os.path.exists(path):
#         return
#     for f in Path(path).glob("*"):
#         if f.is_file():
#             f.unlink()
#         elif f.is_dir():
#             shutil.rmtree(f)

# def unzip_file(source: Path, dest: Path) -> Path:
#     """Unzip .gz recursively"""
#     current = source
#     dest.mkdir(parents=True, exist_ok=True)

#     while current.suffix == ".gz":
#         out_file = dest / current.stem

#         # üö® FILE PROTECTION
#         if out_file.exists():
#             return out_file
#         with gzip.open(current, "rb") as f_in, open(out_file, "wb") as f_out:
#             shutil.copyfileobj(f_in, f_out)
#         current = out_file

#     return current


# def convert_xls_to_csv(file_path: Path, dest_path: Path):
#     """Placeholder for XLS/XLSX ‚Üí CSV logic (can use pandas or openpyxl)"""
#     import pandas as pd
#     df = pd.read_excel(file_path)
#     df.to_csv(dest_path, index=False)

# def send_status(msg_type: str, msg: str):
#     """Simulate WebSocket message"""
#     logging.info(f"[{msg_type}] {msg}")

# # --------------------
# # Database Processing
# # --------------------
# def get_dynamic_script(file_name: str, file_path: str) -> list[str]:

#     if file_name.upper() == "MONTRIAL":

#         return [

#             # 1Ô∏è‚É£ Truncate
#             "TRUNCATE TABLE mon_trial_daily;",

#             # 2Ô∏è‚É£ Reset sequence
#             "ALTER SEQUENCE mon_trial_daily_rno_seq RESTART WITH 1;",

#             # 3Ô∏è‚É£ COPY (file_path used here clearly)
#             f"""
#             COPY mon_trial_daily(fulltext)
#             FROM STDIN
#             WITH (
#                 FORMAT text,
#                 ENCODING 'UTF8'
#             );
#             """,

#             # 4Ô∏è‚É£ Cleanup deletes
#             "delete from mon_trial_daily where substr(fulltext,3,10) ='B: GLAJ02 ';",
#             "delete from mon_trial_daily where substr(fulltext,3,10) ='EQUEST PAR';",
#             "delete from mon_trial_daily where substr(fulltext,3,10) ='PORT      ';",
#             "delete from mon_trial_daily where substr(fulltext,3,10) ='TE CODE   ';",
#             "delete from mon_trial_daily where substr(fulltext,3,10) ='QUESTOR   ';",
#             "delete from mon_trial_daily where substr(fulltext,3,10) ='TE        ';",
#             "delete from mon_trial_daily where substr(fulltext,3,10) ='AR        ';",
#             "delete from mon_trial_daily where trim(fulltext) ='01';",
#             "delete from mon_trial_daily where substr(trim(fulltext),1,6) ='CURREN';",
#             "delete from mon_trial_daily where SUBSTR(fulltext,51,8) ='FOR YEAR';",
#             "delete from mon_trial_daily where SUBSTR(fulltext,13,12) ='1          N';",
#             "delete from mon_trial_daily where SUBSTR(fulltext,13,10) ='UNIT TOTAL';",
#             "delete from mon_trial_daily where SUBSTR(fulltext,13,14) ='CURRENCY TOTAL';",
#             "delete from mon_trial_daily where SUBSTR(fulltext,13,12) ='ENTITY TOTAL';",
#             "delete from mon_trial_daily where fulltext IS NULL;",
#             "delete from mon_trial_daily where trim(fulltext)='.00';",
#             "delete from mon_trial_daily where trim(fulltext)='UNIT TOTAL';",

#             # 5Ô∏è‚É£ Procedure
#             "CALL mon_trial_load();",

#             # 6Ô∏è‚É£ Final insert
#             """
#             INSERT INTO MONTRIAL
#             (BRCODE, CGL, CGLDESC, OPEN_BAL, DEBIT_BAL, CREDIT_BAL, NET_CHANGE, END_BAL, DATE_OF_REPORT)
#             SELECT
#                 BRCD::NUMERIC,
#                 CGL::NUMERIC,
#                 CGLDESC,
#                 OPEN_BAL,
#                 DEBIT_BAL,
#                 CREDIT_BAL,
#                 NET_CHANGE,
#                 END_BAL,
#                 REPORT_DT
#             FROM MON_TRIAL_DAILY
#             WHERE END_BAL IS NOT NULL;
#             """,

#             # 7Ô∏è‚É£ Final procedure
#             "CALL mon_trial_drcr();"
#         ]

#     else:
#         raise ValueError("No DB script for file")


# def get_schema_for_branch(branch_code: str) -> str:
#     raw = os.getenv("BRANCH_SCHEMA_MAP", "")
#     if not raw:
#         raise RuntimeError("BRANCH_SCHEMA_MAP not set in .env")

#     mappings = dict(item.split(":") for item in raw.split(","))
    
#     schema = mappings.get(branch_code)
#     if not schema:
#         raise RuntimeError(f"No schema mapped for branch {branch_code}")

#     return schema

# def run_db_script(file_path: str, file_name: str):    
#     try:
#         sqls = get_dynamic_script(file_name, file_path)

#         conn = psycopg2.connect(
#             host=DB_HOST,
#             port=DB_PORT,
#             database=DB_NAME,
#             user=DB_USER,
#             password=DB_PASSWORD
#         )
#         cur = conn.cursor()

#         schema = os.getenv("BRANCH_SCHEMA", "public")
#         cur.execute(f"SET search_path TO {schema}")

#         for sql in sqls:
#             sql_clean = sql.strip()  # removes spaces/newlines

#             # ‚úÖ Handle COPY safely
#             if sql_clean.upper().startswith("COPY"):
#                 with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
#                     # Optional: clean nulls and \r
#                     cleaned = []
#                     for line in f:
#                         line = line.replace('\r', '').replace('\x00', '')
#                         cleaned.append(line.rstrip())

#                 buffer = io.StringIO("\n".join(cleaned))

#                 cur.copy_expert(sql_clean, buffer)

#             else:
#                 # Normal SQL
#                 cur.execute(sql_clean)

#         conn.commit()
#         cur.close()
#         conn.close()

#         msg = f"‚úÖ DB Processing completed for {file_name} ({schema})"
#         send_status("success", msg)
#         return msg

#     except Exception as e:
#         send_status("error", f"‚ùå DB error {file_name}: {e}")
#         raise


# async def process_file(file: FileItem, request: FileRequest):
#     matched_files = []
#     start = datetime.strptime(request.startDate, "%Y-%m-%d").date()
#     end = datetime.strptime(request.endDate, "%Y-%m-%d").date()
#     src_dir = Path(request.fromPath)
#     dest_dir = Path(request.toPath)
#     dest_dir.mkdir(parents=True, exist_ok=True)

#     if src_dir.resolve() == dest_dir.resolve():
#         raise RuntimeError("fromPath and toPath must be different folders")

#     # Here just simulate date folder filtering
#     for folder in os.listdir(src_dir):
#         folder_date = None
#         try:
#             folder_date = datetime.strptime(folder, "%Y%m%d").date()
#         except:
#             continue
#         if not (start <= folder_date <= end):
#             continue

#         folder_path = src_dir / folder  #ERROR COMING FROM HERE....
#         for f in folder_path.glob("*"):
#             if file.file_type == "prefix" and f.name.lower().startswith(file.key.lower()):
#                 actual_file = f
#                 if f.suffix == ".gz":
#                     actual_file = unzip_file(f, dest_dir)
#                 dest_file = dest_dir / actual_file.name
#                 if actual_file.suffix in [".xls", ".xlsx"]:
#                     dest_file = dest_dir / f"{actual_file.stem}.csv"
#                     convert_xls_to_csv(actual_file, dest_file)
#                 else:
#                     if actual_file.parent.resolve() != dest_dir.resolve():
#                         shutil.copy(actual_file, dest_file)
#                 # Process DB (sync for simplicity, async can be added)
#                 result = run_db_script(str(dest_file), file.display_name)
#                 matched_files.append(result)
#                 if file.files == 1:
#                     break
#     if not matched_files:
#         msg = f"‚ùå No matching file for {file.display_name}"
#         send_status("error", msg)
#         matched_files.append(msg)
#     return matched_files

# # --------------------
# # Endpoint
# # --------------------
# @app.post("/copy")
# async def copy_endpoint(request: FileRequest, background_tasks: BackgroundTasks):
#     if not request.files:
#         raise HTTPException(status_code=400, detail="At least one file must be selected")
#     delete_all_files_in_directory(request.toPath)

#     results = await asyncio.gather(*(process_file(f, request) for f in request.files))

#     # Dashboard refresh for special files
#     if any(f.name.lower() == "montrial" for f in request.files):
#         background_tasks.add_task(lambda: send_status("info", "Dashboard refreshed"))

#     # Flatten results
#     flat_results = [item for sublist in results for item in sublist]
#     return flat_results

# # =========================================================
# # REPORTS END POINT 
# # =========================================================

# QUERY_MAP = {
#     "GEN001": """
#         select
#           ROW_NUMBER() OVER () AS serial_no,
#           report_date,
#           report_type,
#           procedure_name,
#           file_name,
#           inserted_records,
#           to_char(execution_time AT TIME ZONE 'Asia/Kolkata',
#                   'DD Mon YYYY HH12:MI:SS AM') as execution_time,
#           to_char(end_time AT TIME ZONE 'Asia/Kolkata',
#                   'DD Mon YYYY HH12:MI:SS AM') as end_time,
#           status,
#           round(extract(epoch from (end_time - execution_time))::numeric, 2)
#             as time_in_seconds,
#           error_count,
#           count(*) over() as total_rows
#         from procedure_log
#         where report_date between %s and %s
#     """
# }

# # =========================================================
# # REQUEST MODELS
# # =========================================================

# class Filter(BaseModel):
#     id: str
#     value: str


# class ReportRequest(BaseModel):
#     reportType: str
#     branch: str
#     fromDate: str
#     toDate: str
#     page: int = 1
#     size: int = 10
#     sortBy: Optional[str] = None
#     sortDirection: Optional[str] = "ASC"
#     filters: Optional[List[Filter]] = []


# # =========================================================
# # UTILS
# # =========================================================

# def format_date(date_str: str) -> str:
#     return datetime.strptime(date_str, "%Y-%m-%d").strftime("%d-%m-%Y")


# def apply_filters(query: str, filters: List[Filter]) -> str:
#     if not filters:
#         return query

#     clauses = []
#     for f in filters:
#         clauses.append(
#             f"CAST({f.id} AS TEXT) LIKE '%{f.value}%'"
#         )

#     return f"select * from ({query}) t WHERE " + " AND ".join(clauses)


# def add_pagination_sorting(
#     query: str,
#     page: int,
#     size: int,
#     sort_by: Optional[str],
#     sort_dir: Optional[str]
# ) -> str:

#     if sort_by:
#         query += f" ORDER BY {sort_by} {sort_dir}"

#     # HERE: size = -1 means no pagination ================================= changes if you want to
#     if size != -1:
#         offset = max((page - 1) * size, 0)
#         query += f" LIMIT {size} OFFSET {offset}"

#     return query


# # =========================================================
# # SINGLE /reports ENDPOINT
# # =========================================================

# @app.post("/reports")
# def get_reports(request: ReportRequest) -> List[Dict[str, Any]]:
#     try:
#         # -------------------------------------------------
#         # Branch validation (kept compatible)
#         # -------------------------------------------------
#         if request.branch == "":
#             raise ValueError("Branch is empty")

#         # HERE: Branch parsed but NOT used in GEN001 query
#         branch = int(request.branch)

#         # -------------------------------------------------
#         # Get SQL
#         # -------------------------------------------------
#         base_query = QUERY_MAP.get(request.reportType)
#         if not base_query:
#             raise HTTPException(status_code=400, detail="Invalid report type")

#         # -------------------------------------------------
#         # Date formatting
#         # -------------------------------------------------
#         from_date = format_date(request.fromDate) if request.fromDate else None
#         to_date = format_date(request.toDate)

#         # -------------------------------------------------
#         # Filters
#         # -------------------------------------------------
#         final_query = apply_filters(base_query, request.filters)

#         # -------------------------------------------------
#         # Pagination + Sorting
#         # -------------------------------------------------
#         final_query = add_pagination_sorting(
#             final_query,
#             request.page,
#             request.size,
#             request.sortBy,
#             request.sortDirection
#         )

#         # -------------------------------------------------
#         # DB EXECUTION
#         # -------------------------------------------------
#         # HERE: Reuse your existing DB config
#         conn = psycopg2.connect(
#             host=DB_HOST,
#             port=DB_PORT,
#             database=DB_NAME,
#             user=DB_USER,
#             password=DB_PASSWORD
#         )

#         try:
#             with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
#                 cur.execute(final_query, (from_date, to_date))
#                 result = cur.fetchall()
#                 return result
#         finally:
#             conn.close()

#     except psycopg2.errors.QueryCanceled:
#         logger.error("Query timeout")
#         raise HTTPException(
#             status_code=408,
#             detail="Request timed out. Please try again with different filters."
#         )

#     except psycopg2.errors.SyntaxError as e:
#         logger.error(str(e))
#         raise HTTPException(status_code=404, detail=str(e))

#     except ValueError:
#         raise HTTPException(status_code=400, detail="Invalid Arguments")

#     except Exception as e:
#         logger.error(str(e))
#         raise HTTPException(
#             status_code=500,
#             detail=f"Unexpected error: {str(e)}"
#         )

# if __name__ == "__main__":
#     import uvicorn
#     uvicorn.run(app, host="0.0.0.0", port=5000, reload=True)